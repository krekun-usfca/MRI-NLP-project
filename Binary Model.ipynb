{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>text</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>58522792</td>\n",
       "      <td>16567081</td>\n",
       "      <td>b\"                                 FINAL REPOR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>58213163</td>\n",
       "      <td>16567081</td>\n",
       "      <td>b'                                 FINAL REPOR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>59835582</td>\n",
       "      <td>16043746</td>\n",
       "      <td>b'                                 FINAL REPOR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>51487790</td>\n",
       "      <td>16456872</td>\n",
       "      <td>b'                                 FINAL REPOR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>59750073</td>\n",
       "      <td>16824069</td>\n",
       "      <td>b'                                 FINAL REPOR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id  subject_id                                               text  \\\n",
       "0  58522792    16567081  b\"                                 FINAL REPOR...   \n",
       "1  58213163    16567081  b'                                 FINAL REPOR...   \n",
       "2  59835582    16043746  b'                                 FINAL REPOR...   \n",
       "3  51487790    16456872  b'                                 FINAL REPOR...   \n",
       "4  59750073    16824069  b'                                 FINAL REPOR...   \n",
       "\n",
       "   Atelectasis  Cardiomegaly  Consolidation  Edema  \\\n",
       "0          0.0           0.0            0.0    0.0   \n",
       "1          0.0           1.0            0.0    1.0   \n",
       "2          0.0           0.0            0.0    0.0   \n",
       "3          0.0           0.0            0.0    0.0   \n",
       "4          0.0           0.0            0.0    0.0   \n",
       "\n",
       "   Enlarged Cardiomediastinum  Fracture  Lung Lesion  Lung Opacity  \\\n",
       "0                         0.0       0.0          0.0           0.0   \n",
       "1                         0.0       0.0          0.0           0.0   \n",
       "2                         0.0       0.0          0.0           0.0   \n",
       "3                         0.0       0.0          0.0           0.0   \n",
       "4                         0.0       0.0          0.0           0.0   \n",
       "\n",
       "   No Finding  Pleural Effusion  Pleural Other  Pneumonia  Pneumothorax  \\\n",
       "0         0.0               0.0            0.0        1.0           0.0   \n",
       "1         0.0               1.0            0.0        0.0           0.0   \n",
       "2         1.0               0.0            0.0        0.0           0.0   \n",
       "3         1.0               0.0            0.0        0.0           0.0   \n",
       "4         1.0               0.0            0.0        0.0           0.0   \n",
       "\n",
       "   Support Devices  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_data = pd.read_csv(\"data/text_binary.csv\")\n",
    "mimic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>No Finding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b\"                                 final repor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  No Finding\n",
       "0  b\"                                 final repor...         0.0\n",
       "1  b'                                 final repor...         0.0\n",
       "2  b'                                 final repor...         1.0\n",
       "3  b'                                 final repor...         1.0\n",
       "4  b'                                 final repor...         1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_data = mimic_data[['text','No Finding']]\n",
    "binary_data.text = binary_data.text.str.lower()\n",
    "binary_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Finding</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>152372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>75455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text\n",
       "No Finding        \n",
       "0.0         152372\n",
       "1.0          75455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_data.groupby('No Finding').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"                                 final report\\\\n type of examination:  chest pa and lateral.\\\\n \\\\n indication:  ___-year-old male patient with recent pneumonia diagnosed and\\\\n treated at another facility.  x-ray not available, now with continued cough\\\\n and wheeze, history of copd, remaining evidence of pneumonia?\\\\n \\\\n findings:  pa and lateral chest views were obtained with patient in upright\\\\n position.  analysis is performed in direct comparison with the next preceding\\\\n chest examination of ___.  the heart size remains normal.  no\\\\n typical configurational abnormality is seen.  the thoracic aorta is moderately\\\\n widened and somewhat elongated but no local contour abnormalities are\\\\n identified.  the pulmonary vasculature is not congested.  there exists,\\\\n however, some irregular peripheral vascular distribution most marked on the\\\\n bases and coinciding with some slightly hypertranslucent pulmonary areas and\\\\n flattened low positioned diaphragms are indicative of copd.  when direct\\\\n comparison is made with the previous examination of ___, there is a hazy mild\\\\n degree of density in the left base shows a corresponding local thickening of\\\\n the lower portion of the major fissure on the left side.  this finding is\\\\n consistent with some resolving pneumonic process such as recent pneumonia in\\\\n resolution.  skeletal changes which were characterized by some longitudinal\\\\n calcification in the anterior ligaments of the thoracic spine appears stable\\\\n and has not progressed.  no new skeletal abnormalities identified.\\\\n \\\\n impression:  findings of general copd as before, minor pleural and parenchymal\\\\n hazy densities on the left base probably pneumonia in resolution and match the\\\\n clinical report of a recent pneumonia.  if patient\\'s symptoms continue,\\\\n recommend another follow up examination in a week or so.\\\\n\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = binary_data['No Finding']\n",
    "X = binary_data.drop(columns=['No Finding'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  b'                                 final repor...\n",
       "1  b'                                 final repor...\n",
       "2  b'                                 final repor...\n",
       "3  b'                                 final repor...\n",
       "4  b'                                 final repor..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all '\\\\n' from the text\n",
    "re_newlines = re.compile('\\\\\\\\n')\n",
    "def sub_newlines(x): return re_newlines.sub('',x)\n",
    "\n",
    "# remove all special characters from the text, keep only alphanumeric and spaces\n",
    "re_letters = re.compile('[^A-Za-z0-9 ]')\n",
    "def sub_letters(x): return re_letters.sub('', x)\n",
    "\n",
    "# remove excessive spacing otherwise you end up with \" \" substrings\n",
    "re_spaces = re.compile('\\s+')\n",
    "def sub_spaces(x): return re_spaces.sub(' ', x)\n",
    "                \n",
    "# tokenize all words.\n",
    "my_tok = spacy.load('en')\n",
    "def spacy_tok(x): \n",
    "    return [tok.text for tok in my_tok.tokenizer(sub_spaces\n",
    "                                                 (sub_letters\n",
    "                                                 (sub_newlines(x))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b final report type of examination chest pa and lateral indication yearold male patient with recent pneumonia diagnosed and treated at another facility xray not available now with continued cough and wheeze history of copd remaining evidence of pneumonia findings pa and lateral chest views were obtained with patient in upright position analysis is performed in direct comparison with the next preceding chest examination of the heart size remains normal no typical configurational abnormality is seen the thoracic aorta is moderately widened and somewhat elongated but no local contour abnormalities are identified the pulmonary vasculature is not congested there exists however some irregular peripheral vascular distribution most marked on the bases and coinciding with some slightly hypertranslucent pulmonary areas and flattened low positioned diaphragms are indicative of copd when direct comparison is made with the previous examination of there is a hazy mild degree of density in the left base shows a corresponding local thickening of the lower portion of the major fissure on the left side this finding is consistent with some resolving pneumonic process such as recent pneumonia in resolution skeletal changes which were characterized by some longitudinal calcification in the anterior ligaments of the thoracic spine appears stable and has not progressed no new skeletal abnormalities identified impression findings of general copd as before minor pleural and parenchymal hazy densities on the left base probably pneumonia in resolution and match the clinical report of a recent pneumonia if patients symptoms continue recommend another follow up examination in a week or so'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_spaces(sub_letters(sub_newlines(binary_data.text[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final',\n",
       " 'report',\n",
       " 'type',\n",
       " 'of',\n",
       " 'examination',\n",
       " 'chest',\n",
       " 'pa',\n",
       " 'and',\n",
       " 'lateral',\n",
       " 'indication',\n",
       " 'yearold',\n",
       " 'male',\n",
       " 'patient',\n",
       " 'with',\n",
       " 'recent',\n",
       " 'pneumonia',\n",
       " 'diagnosed',\n",
       " 'and',\n",
       " 'treated',\n",
       " 'at',\n",
       " 'another',\n",
       " 'facility',\n",
       " 'xray',\n",
       " 'not',\n",
       " 'available',\n",
       " 'now',\n",
       " 'with',\n",
       " 'continued',\n",
       " 'cough',\n",
       " 'and',\n",
       " 'wheeze',\n",
       " 'history',\n",
       " 'of',\n",
       " 'copd',\n",
       " 'remaining',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'pneumonia',\n",
       " 'findings',\n",
       " 'pa',\n",
       " 'and',\n",
       " 'lateral',\n",
       " 'chest',\n",
       " 'views',\n",
       " 'were',\n",
       " 'obtained',\n",
       " 'with',\n",
       " 'patient',\n",
       " 'in',\n",
       " 'upright',\n",
       " 'position',\n",
       " 'analysis',\n",
       " 'is',\n",
       " 'performed',\n",
       " 'in',\n",
       " 'direct',\n",
       " 'comparison',\n",
       " 'with',\n",
       " 'the',\n",
       " 'next',\n",
       " 'preceding',\n",
       " 'chest',\n",
       " 'examination',\n",
       " 'of',\n",
       " 'the',\n",
       " 'heart',\n",
       " 'size',\n",
       " 'remains',\n",
       " 'normal',\n",
       " 'no',\n",
       " 'typical',\n",
       " 'configurational',\n",
       " 'abnormality',\n",
       " 'is',\n",
       " 'seen',\n",
       " 'the',\n",
       " 'thoracic',\n",
       " 'aorta',\n",
       " 'is',\n",
       " 'moderately',\n",
       " 'widened',\n",
       " 'and',\n",
       " 'somewhat',\n",
       " 'elongated',\n",
       " 'but',\n",
       " 'no',\n",
       " 'local',\n",
       " 'contour',\n",
       " 'abnormalities',\n",
       " 'are',\n",
       " 'identified',\n",
       " 'the',\n",
       " 'pulmonary',\n",
       " 'vasculature',\n",
       " 'is',\n",
       " 'not',\n",
       " 'congested',\n",
       " 'there',\n",
       " 'exists',\n",
       " 'however',\n",
       " 'some',\n",
       " 'irregular',\n",
       " 'peripheral',\n",
       " 'vascular',\n",
       " 'distribution',\n",
       " 'most',\n",
       " 'marked',\n",
       " 'on',\n",
       " 'the',\n",
       " 'bases',\n",
       " 'and',\n",
       " 'coinciding',\n",
       " 'with',\n",
       " 'some',\n",
       " 'slightly',\n",
       " 'hypertranslucent',\n",
       " 'pulmonary',\n",
       " 'areas',\n",
       " 'and',\n",
       " 'flattened',\n",
       " 'low',\n",
       " 'positioned',\n",
       " 'diaphragms',\n",
       " 'are',\n",
       " 'indicative',\n",
       " 'of',\n",
       " 'copd',\n",
       " 'when',\n",
       " 'direct',\n",
       " 'comparison',\n",
       " 'is',\n",
       " 'made',\n",
       " 'with',\n",
       " 'the',\n",
       " 'previous',\n",
       " 'examination',\n",
       " 'of',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'hazy',\n",
       " 'mild',\n",
       " 'degree',\n",
       " 'of',\n",
       " 'density',\n",
       " 'in',\n",
       " 'the',\n",
       " 'left',\n",
       " 'base',\n",
       " 'shows',\n",
       " 'a',\n",
       " 'corresponding',\n",
       " 'local',\n",
       " 'thickening',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lower',\n",
       " 'portion',\n",
       " 'of',\n",
       " 'the',\n",
       " 'major',\n",
       " 'fissure',\n",
       " 'on',\n",
       " 'the',\n",
       " 'left',\n",
       " 'side',\n",
       " 'this',\n",
       " 'finding',\n",
       " 'is',\n",
       " 'consistent',\n",
       " 'with',\n",
       " 'some',\n",
       " 'resolving',\n",
       " 'pneumonic',\n",
       " 'process',\n",
       " 'such',\n",
       " 'as',\n",
       " 'recent',\n",
       " 'pneumonia',\n",
       " 'in',\n",
       " 'resolution',\n",
       " 'skeletal',\n",
       " 'changes',\n",
       " 'which',\n",
       " 'were',\n",
       " 'characterized',\n",
       " 'by',\n",
       " 'some',\n",
       " 'longitudinal',\n",
       " 'calcification',\n",
       " 'in',\n",
       " 'the',\n",
       " 'anterior',\n",
       " 'ligaments',\n",
       " 'of',\n",
       " 'the',\n",
       " 'thoracic',\n",
       " 'spine',\n",
       " 'appears',\n",
       " 'stable',\n",
       " 'and',\n",
       " 'has',\n",
       " 'not',\n",
       " 'progressed',\n",
       " 'no',\n",
       " 'new',\n",
       " 'skeletal',\n",
       " 'abnormalities',\n",
       " 'identified',\n",
       " 'impression',\n",
       " 'findings',\n",
       " 'of',\n",
       " 'general',\n",
       " 'copd',\n",
       " 'as',\n",
       " 'before',\n",
       " 'minor',\n",
       " 'pleural',\n",
       " 'and',\n",
       " 'parenchymal',\n",
       " 'hazy',\n",
       " 'densities',\n",
       " 'on',\n",
       " 'the',\n",
       " 'left',\n",
       " 'base',\n",
       " 'probably',\n",
       " 'pneumonia',\n",
       " 'in',\n",
       " 'resolution',\n",
       " 'and',\n",
       " 'match',\n",
       " 'the',\n",
       " 'clinical',\n",
       " 'report',\n",
       " 'of',\n",
       " 'a',\n",
       " 'recent',\n",
       " 'pneumonia',\n",
       " 'if',\n",
       " 'patients',\n",
       " 'symptoms',\n",
       " 'continue',\n",
       " 'recommend',\n",
       " 'another',\n",
       " 'follow',\n",
       " 'up',\n",
       " 'examination',\n",
       " 'in',\n",
       " 'a',\n",
       " 'week',\n",
       " 'or',\n",
       " 'so']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tok(binary_data.text[0])[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>No Finding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b\"                                 final repor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  No Finding\n",
       "0  b\"                                 final repor...         0.0\n",
       "1  b'                                 final repor...         0.0\n",
       "2  b'                                 final repor...         1.0\n",
       "3  b'                                 final repor...         1.0\n",
       "4  b'                                 final repor...         1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_train = binary_data.copy()\n",
    "binary_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(text):\n",
    "    counts = Counter()\n",
    "    for word in text:\n",
    "        counts.update(spacy_tok(word)[1:])\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = get_counts(binary_train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37093"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in list(word_count):\n",
    "    if word_count[word] < 3:\n",
    "        del word_count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15527"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index = {\"<PAD>\":0, \"UNK\":1} # init with padding and unknown\n",
    "words = [\"<PAD>\", \"UNK\"]\n",
    "for word in word_count:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15529"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text):\n",
    "    enc = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in text.split()])\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   1,   4,   5,   1,   7,   8,   9,   1,   1,   1,   1,\n",
       "        13,  14,  15,  16,  17,  18,   1,  19,  20,  21,   1,   1,  24,\n",
       "         1,  26,  15,  27,   1,   9,   1,  30,   5,   1,  32,  33,   5,\n",
       "         1,   1,   1,   8,   9,  10,   7,  35,  36,  37,  15,  14,  38,\n",
       "         1,   1,  41,  42,  43,  38,  44,  45,  15,  46,  47,   1,   7,\n",
       "         6,   5,   1,  46,  49,  50,  51,   1,   1,  54,  55,  56,  42,\n",
       "         1,  46,  58,  59,  42,   1,  61,   9,  62,  63,  64,  53,  65,\n",
       "        66,  67,   1,   1,  46,  70,  71,  42,  24,   1,  73,   1,   1,\n",
       "        76,  77,  78,  79,  80,  81,  82,  83,   1,  84,   9,  85,  15,\n",
       "        76,  86,  87,  70,  88,   1,  89,  90,  91,  92,  68,  93,   5,\n",
       "         1,  94,   1,  45,  42,  95,  15,  46,  96,   6,   5,   1,  73,\n",
       "        42,  97,  98,   1, 100,   5, 101,  38,  46, 102, 103, 104,  97,\n",
       "       105,  65, 106,   1,  46, 107, 108,   5,  46, 109, 110,  83,  46,\n",
       "       102,   1, 112, 113,   1, 114,  15,  76, 115, 116, 117, 118, 119,\n",
       "        16,  17,   1,   1, 121, 122, 123,  36, 124, 125,  76,   1, 127,\n",
       "        38,  46, 128,   1,   5,  46,  58, 129, 130,   1,   9, 132,  24,\n",
       "         1,  53, 134, 121,  67,   1,   1,   1,  34,   5, 136,  31, 119,\n",
       "         1, 138, 139,   9,   1,  98, 141,  83,  46, 102, 103, 142,  17,\n",
       "        38, 120,   9, 143,   1, 144,   3,   5,  97,  16,   1, 145,   1,\n",
       "       147,   1, 149,  21, 150, 151,   6,  38,  97, 152, 153,   1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(binary_train.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_Mimic(Dataset):\n",
    "    def __init__(self, X, y, vocab):\n",
    "        self.x = [encode_sentence(x) for x in X.text]\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train = Binary_Mimic(X_train, y_train, vocab2index)\n",
    "b_valid = Binary_Mimic(X_val, y_val, vocab2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (sentences, labels).\n",
    "    \n",
    "    Need custom collate_fn because merging sequences (including padding) is not \n",
    "    supported in default. Sequences are padded to the maximum length of mini-batch \n",
    "    sequences (dynamic padding).\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuple (sentence, label). \n",
    "            - list of word indices of variable length\n",
    "            - label, 0 or 1\n",
    "    Returns:\n",
    "        packed_batch: (PackedSequence), see torch.nn.utils.rnn.pack_padded_sequence\n",
    "        sencences: torch tensor of shape (batch_size, max_len).\n",
    "        labels: torch tensor of shape (batch_size, 1).\n",
    "        lengths: list; valid length for each padded sentence. \n",
    "    \"\"\"\n",
    "    # Sort a data list by sentences length (descending order).\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    sentences, labels = zip(*data)\n",
    "    \n",
    "    # stack labels\n",
    "    labels = torch.Tensor(labels)\n",
    "    \n",
    "    # Merge sentences\n",
    "    lengths = [len(s) for s in sentences]\n",
    "   \n",
    "    sents = torch.zeros(len(sentences), max(lengths)).long()\n",
    "    for i, s in enumerate(sentences):\n",
    "        end = lengths[i]\n",
    "        sents[i, :end] = torch.Tensor(s[:end])        \n",
    "    \n",
    "    return sents, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([   1,    2,    1,    1,    7,    1,    1,    1,    1,    1,  156,\n",
       "          157,  517,   15,    1,    1,  565,    1,    1, 1147,    1,    1,\n",
       "         7051, 1956,    5,   70,    1,    1,    1,    1,    1,  119,  250,\n",
       "          165,    1,    7,    1,   73,  132,  310,  318,    1,  319,   38,\n",
       "           46,  324,    5,   46,    7, 1798,  190,  817,  300, 2071,    1,\n",
       "           46,  238,    1]), 1.0),\n",
       " (array([   1,  283,    1,    1,    1,    1,    1,    1,   53,  191,  204,\n",
       "            1,   53,   33,    5,  191, 1396,   38,   46,    1,    1, 1002,\n",
       "          190,  844,  741,    1, 1658,  844,  194,  468,    1,    1,  283,\n",
       "          284,  510,    1,    1,    1,    1,    1,    1,   99,  387,   38,\n",
       "           46,  699,  168,  238,  103,  442,  443,    1,   64,    1,  485,\n",
       "          494,  153,   17,   38,   46,  168,  144,    1,    1,    2,    1,\n",
       "            1,    7,    1,    1,    1,    1,    1,   15, 1490,   16,    1,\n",
       "         1351, 1752,    1,  238,    1,   83, 1004,    1,  641,    1,    1,\n",
       "            1,    1,    7,    8,    9,    1,    1,    1,   45,   42,   95,\n",
       "            7,  194,  182,    1,    1,    1,    1,    1,    1,    1,   46,\n",
       "          200,   68,  461,  492,    9,    1,   73,   42,   53,  139,  163,\n",
       "            1,    1,   46,  224,  225,   42,    1,   53,    1,    5,  191,\n",
       "         1396,   42,   57,   38,   46,    1,    1,    1,    1,    1,   53,\n",
       "          191,  204,    1,   53,   33,    5,  191, 1396,   38,   46,    1,\n",
       "            1,  145, 1002,  190,  844,  741,    1, 1658,  844,  194,  468,\n",
       "            1,    1]), 0.0),\n",
       " (array([   1,    2,    1,    1,    1,  156,  157,  517,   15,  139,    1,\n",
       "           16,    7,    1,    1,    9,  134,  423,    5,  238,    1,    1,\n",
       "           42,  163,    1,  271,   33,    5, 1416,  943, 1118,    7, 1143,\n",
       "            7,  259,  447,    1,    1,    1,    7,    8,    9,    1,    1,\n",
       "            1,    7,  174,  425,    1,    1,    1,    1,    1,   46,  102,\n",
       "          407,  165,  107,  238,  334,   26,  132,   97, 5036,    1,    1,\n",
       "          376, 1982,   97,  810,    5, 2114,  550,  182, 1039,    5,    1,\n",
       "         2311,    5,   46,  107,  381,    9,  162,  165,  167,  139,  163,\n",
       "          948,    1, 2333,  139,    1,   46,   32,  102,  238,   42,  202,\n",
       "            1,    1,   46,  168,  238,   42,  202,  211,    1,   46,  560,\n",
       "            1,   70, 4990,   68, 1425,    1,   53,    1,    1,  224,  225,\n",
       "           42,    1,   53,    1,    1,    1,    1,    1,   46,  280,  102,\n",
       "          238,  334,  376, 1982,   97,  810,    5,    1,  550,    9,  162,\n",
       "          165,  167,  139,  163,  948,  172,    1,  139,  163,    1]), 0.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [b_train[0], b_train[1], b_train[2]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   1,  283,    1,    1,    1,    1,    1,    1,   53,  191,  204,    1,\n",
       "            53,   33,    5,  191, 1396,   38,   46,    1,    1, 1002,  190,  844,\n",
       "           741,    1, 1658,  844,  194,  468,    1,    1,  283,  284,  510,    1,\n",
       "             1,    1,    1,    1,    1,   99,  387,   38,   46,  699,  168,  238,\n",
       "           103,  442,  443,    1,   64,    1,  485,  494,  153,   17,   38,   46,\n",
       "           168,  144,    1,    1,    2,    1,    1,    7,    1,    1,    1,    1,\n",
       "             1,   15, 1490,   16,    1, 1351, 1752,    1,  238,    1,   83, 1004,\n",
       "             1,  641,    1,    1,    1,    1,    7,    8,    9,    1,    1,    1,\n",
       "            45,   42,   95,    7,  194,  182,    1,    1,    1,    1,    1,    1,\n",
       "             1,   46,  200,   68,  461,  492,    9,    1,   73,   42,   53,  139,\n",
       "           163,    1,    1,   46,  224,  225,   42,    1,   53,    1,    5,  191,\n",
       "          1396,   42,   57,   38,   46,    1,    1,    1,    1,    1,   53,  191,\n",
       "           204,    1,   53,   33,    5,  191, 1396,   38,   46,    1,    1,  145,\n",
       "          1002,  190,  844,  741,    1, 1658,  844,  194,  468,    1,    1],\n",
       "         [   1,    2,    1,    1,    1,  156,  157,  517,   15,  139,    1,   16,\n",
       "             7,    1,    1,    9,  134,  423,    5,  238,    1,    1,   42,  163,\n",
       "             1,  271,   33,    5, 1416,  943, 1118,    7, 1143,    7,  259,  447,\n",
       "             1,    1,    1,    7,    8,    9,    1,    1,    1,    7,  174,  425,\n",
       "             1,    1,    1,    1,    1,   46,  102,  407,  165,  107,  238,  334,\n",
       "            26,  132,   97, 5036,    1,    1,  376, 1982,   97,  810,    5, 2114,\n",
       "           550,  182, 1039,    5,    1, 2311,    5,   46,  107,  381,    9,  162,\n",
       "           165,  167,  139,  163,  948,    1, 2333,  139,    1,   46,   32,  102,\n",
       "           238,   42,  202,    1,    1,   46,  168,  238,   42,  202,  211,    1,\n",
       "            46,  560,    1,   70, 4990,   68, 1425,    1,   53,    1,    1,  224,\n",
       "           225,   42,    1,   53,    1,    1,    1,    1,    1,   46,  280,  102,\n",
       "           238,  334,  376, 1982,   97,  810,    5,    1,  550,    9,  162,  165,\n",
       "           167,  139,  163,  948,  172,    1,  139,  163,    1,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [   1,    2,    1,    1,    7,    1,    1,    1,    1,    1,  156,  157,\n",
       "           517,   15,    1,    1,  565,    1,    1, 1147,    1,    1, 7051, 1956,\n",
       "             5,   70,    1,    1,    1,    1,    1,  119,  250,  165,    1,    7,\n",
       "             1,   73,  132,  310,  318,    1,  319,   38,   46,  324,    5,   46,\n",
       "             7, 1798,  190,  817,  300, 2071,    1,   46,  238,    1,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " [167, 153, 58],\n",
       " tensor([0., 0., 1.]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     2,     1,     1,     7,     1,     9,     1,     1,     1,\n",
       "             1,    15,     1,     1,   189,   190,   191,     1,     1,     1,\n",
       "             7,     8,     9,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,    46,    49,     1,     1,     9,   198,   199,    68,     1,\n",
       "            46,   200,     1,   202,   211,   139,     1,   212,     1,   153,\n",
       "             1,     1,     1,     1,     1,    53,   191,   204,     1],\n",
       "        [    1,     2,     1,     1,     7,     1,     1,     1, 11398,   452,\n",
       "             1,  1642,     1,     1,     1,     7,     8,     9,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,    49,    50,    42,     1,\n",
       "           224,   225,     9,   198,   199,     1,     1,   200,    68,     1,\n",
       "           139,   780,    68,   202,   211,   163,     1,     1,     1,     1,\n",
       "             1,     1,    52,     7,     1,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(b_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "sents, lengths, labels = next(iter(train_loader))\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 59]), torch.Size([2]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59, 55]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, optimizer, train_dl, valid_dl, epochs=10):\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, s, y in train_dl:\n",
    "            x = x.long()#.cuda()\n",
    "            y = y.float()#.cuda()\n",
    "            y_pred = model(x, s)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc = val_metrics(model, valid_dl)\n",
    "        print(\"Epoch #%.f: train loss %.3f val loss %.3f and val accuracy %.3f\" % \n",
    "              (i+1,sum_loss/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, s, y in valid_dl:\n",
    "        x = x.long()#.cuda()\n",
    "        y = y.float().unsqueeze(1)#.cuda()\n",
    "        y_hat = model(x, s)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_pred = y_hat > 0\n",
    "        correct += (y_pred.float() == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super(GRUModel,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        print(x.shape)\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        pack = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        out_pack, ht = self.gru(pack)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50000\n",
    "train_dl = DataLoader(b_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(b_valid, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15529\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "model = GRUModel(vocab_size, 50, 50)#.cuda()\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epocs(model, optimizer, train_dl, valid_dl, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Glove to create pre-trained embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile='data/glove.6B.50d.txt'):\n",
    "    \"\"\" Loads word vectors into a dictionary.\"\"\"\n",
    "    f = open(gloveFile,'r')\n",
    "    word_vecs = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        word_vecs[word] = np.array([float(val) for val in splitLine[1:]])\n",
    "    return word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = loadGloveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = get_counts(binary_train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 37093\n"
     ]
    }
   ],
   "source": [
    "print(len(word_vecs.keys()), len(word_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rare_words(word_vecs, word_count, min_df=4):\n",
    "    \"\"\" Deletes rare words from word_count\n",
    "    \n",
    "    Deletes words from word_count if they are not in word_vecs\n",
    "    and don't have at least min_df occurrencies in word_count.\n",
    "    \"\"\"\n",
    "    words_delete = []\n",
    "    for word in word_count:\n",
    "        if word_count[word] < min_df and word not in word_vecs:\n",
    "            words_delete.append(word)\n",
    "    for word in words_delete: word_count.pop(word)\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20222\n"
     ]
    }
   ],
   "source": [
    "word_count = delete_rare_words(word_vecs, word_count, min_df=3)\n",
    "print(len(word_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_vecs, word_count, min_df=4, emb_size=50):\n",
    "    \"\"\"Creates embedding matrix from word vectors. \"\"\"\n",
    "    word_count = delete_rare_words(word_vecs, word_count, min_df)\n",
    "    V = len(word_count.keys()) + 2\n",
    "    vocab2index = {}\n",
    "    W = np.zeros((V, emb_size), dtype=\"float32\")\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    # adding a vector for padding\n",
    "    W[0] = np.zeros(emb_size, dtype='float32')\n",
    "    # adding a vector for rare words \n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "    vocab2index[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_count:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1   \n",
    "    return W, np.array(vocab), vocab2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weight, vocab, vocab2index = create_embedding_matrix(word_vecs, word_count, min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20224"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrained_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1946, -0.1763, -0.2316,  ..., -0.0365,  0.2413,  0.1304],\n",
       "        [-0.4124,  0.6493, -0.5585,  ...,  0.2621,  0.1045, -0.4430],\n",
       "        ...,\n",
       "        [-0.1321, -0.3749, -0.5517,  ...,  0.9329,  0.3362, -0.2008],\n",
       "        [ 0.8547,  0.1916,  0.0963,  ...,  0.1707, -0.2616,  0.5350],\n",
       "        [-0.0111, -1.1496, -0.5092,  ...,  0.0186, -0.9912,  0.1102]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = 50\n",
    "V = len(pretrained_weight)\n",
    "emb = nn.Embedding(V, emb_size)\n",
    "emb.weight.data.copy_(torch.from_numpy(pretrained_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Model with Pretrained embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel_pre(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights=None) :\n",
    "        super(GRUModel_pre,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        pack = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        out_pack, ht = self.gru(pack)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "model = GRUModel_pre(vocab_size, 50, 50, glove_weights=pretrained_weight)#.cuda()\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epocs(model, optimizer,train_dl, valid_dl, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super(LSTM,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        out_pack, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs_v0(model, optimizer, train_dl, val_dl, epochs=10):\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, s, y in train_dl:\n",
    "            # s is not used in this model\n",
    "            x = x.long()#.cuda()\n",
    "            y = y.float()#.cuda()\n",
    "            y_pred = model(x)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc = val_metrics_v0(model, val_dl)\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics_v0(model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, s, y in valid_dl:\n",
    "        # s is not used here\n",
    "        x = x.long()#.cuda()\n",
    "        y = y.float().unsqueeze(1)#.cuda()\n",
    "        y_hat = model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_pred = y_hat > 0\n",
    "        correct += (y_pred.float() == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50000\n",
    "train_dl = DataLoader(b_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(b_valid, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15529\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "model = LSTM(vocab_size, 50, 50)\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 527])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-4c9e6b10e68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_epocs_v0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-cc728ac729f6>\u001b[0m in \u001b[0;36mtrain_epocs_v0\u001b[0;34m(model, optimizer, train_dl, val_dl, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-623a9ca3a048>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mout_pack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_epocs_v0(model, optimizer, train_dl, valid_dl, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
